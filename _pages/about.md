---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - "/wordpress/"
  - "/wordpress/index.html"
---

{% include base_path %}

   
Welcome to my homepage! My full name is Huy Minh Nguyen, and I am currently a second-year Ph.D student at the [Department of Statistics and Data Sciecnes, University of Texas at Austin](https://stat.utexas.edu/) where I am fortunate to be advised by Professor [Nhat Ho](https://nhatptnk8912.github.io/) and Professor [Alessandro Rinaldo](https://arinaldo.github.io/). Before that, I graduated from [Ho Chi Minh City University of Science](https://en.hcmus.edu.vn/) with a Bachelor's degree in Mathematics (Summa Cum Laude). 

Email: huynm@utexas.edu
## Research Interests 
My current research focuses on theoretical foundations for Mixture-of-Experts models. In particular, I try to comprehend the effects of various gating functions (namely softmax gate, top-K sparse softmax gate, dense-to-sparse gate, etc) on the convergence of density estimation and parameter estimation under the Mixture-of-Experts models. Based on insights from these results, I aim to design novel gating functions which help improve the performance of Mixture-of-Experts applications, including Large Language Models and Medical Images. Additionally, I am also interested in Optimal Transport theory.

<span style="color:red"> **(\*) denotes equal contribution.** </span> <br/>
## Selected Preprints
### On Least Squares Estimation in Softmax Gating Mixture of Experts
*__Huy Nguyen__, Nhat Ho, Alessandro Rinaldo*<br/>
Under review [[arXiv](https://arxiv.org/abs/2402.02952)]
### Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?
*__Huy Nguyen__, Pedram Akbarian, Nhat Ho*<br/>
Under review [[arXiv](https://arxiv.org/abs/2401.13875)]
### Fuse MoE: Mixture-of-Experts Transformers for Fleximodal Fusion
*Xing Han,__Huy Nguyen\*__, Carl Harris\*, Nhat Ho, Suchi Saria*<br/>
Under review [[arXiv](https://arxiv.org/abs/2402.03226)]
### CompeteSMoE - Effective Training of Sparse Mixture of Experts via Competition
*Quang Pham, Giang Do, __Huy Nguyen__, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, Nhat Ho*<br/>
Under review [[arXiv](https://arxiv.org/abs/2402.02526)]
### A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
*__Huy Nguyen__, Pedram Akbarian, TrungTin Nguyen, Nhat Ho*<br/>
Under review [[arXiv](https://arxiv.org/abs/2310.14188)]

## Selected Publications on Mixture of Experts
### Demystifying Softmax Gating Function in Gaussian Mixture of Experts 
*__Huy Nguyen__, TrungTin Nguyen, Nhat Ho*<br/>
Advances in NeurIPS, 2023  <span style="color:red"> **(Spotlight)** </span>. [[arXiv](https://arxiv.org/abs/2305.03288)] [[NeurIPS](https://openreview.net/pdf?id=cto6jIIbMZ)]
### Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts
*__Huy Nguyen__, Pedram Akbarian, Fanqi Yan, Nhat Ho*<br/>
Proceedings of the ICLR, 2024. [[arXiv](https://arxiv.org/abs/2309.13850)]
### Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models
*Dat Do\*, __Huy Nguyen\*__, Khai Nguyen, Nhat Ho*<br/>
Advances in NeurIPS, 2023. [[arXiv](https://arxiv.org/abs/2301.11808)] [[NeurIPS](https://openreview.net/pdf?id=w3ghbKBJg4)]
### Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts
*__Huy Nguyen\*__, TrungTin Nguyen\*, Khai Nguyen, Nhat Ho*<br/>
In AISTATS, 2024. [[arXiv](https://arxiv.org/abs/2305.07572)]
### On Parameter Estimation in Deviated Gaussian Mixture of Experts
*__Huy Nguyen__, Khai Nguyen, Nhat Ho*<br/>
In AISTATS, 2024.

## Selected Publications on Optimal Transport
### Entropic Gromov-Wasserstein between Gaussian Distributions
*__Huy Nguyen\*__, Khang Le\*, Dung Le\*, Dat Do, Tung Pham, Nhat Ho*<br/>
Proceedings of the ICML, 2022.  [[arXiv](https://arxiv.org/abs/2108.10961)] [[ICML](https://proceedings.mlr.press/v162/le22a.html)]
### On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity
*__Huy Nguyen\*__, Khang Le\*, Khai Nguyen, Tung Pham, Nhat Ho*<br/>
In AISTATS, 2022.  [[arXiv](https://arxiv.org/abs/2108.07992)] [[AISTATS](https://proceedings.mlr.press/v151/le22a.html)]
### On Robust Optimal Transport: Computational Complexity and Barycenter Computation
*__Huy Nguyen\*__, Khang Le\*, Quang Minh Nguyen, Tung Pham, Hung Bui, Nhat Ho*<br/>
Advances in NeurIPS, 2021.  [[arXiv](https://arxiv.org/abs/2102.06857)] [[NeurIPS](https://proceedings.neurips.cc/paper/2021/hash/b80ba73857eed2a36dc7640e2310055a-Abstract.html)]



## Recent News
- **[Feb 2024]** Two new papers on the applications of Mixture of Experts in Medical Images [[1](https://arxiv.org/abs/2402.03226)] and Large Language Models [[2](https://arxiv.org/abs/2402.02526)] are out!
- **[Feb 2024]** Two new papers on the theory of Mixture of Experts, [[1](https://arxiv.org/abs/2402.02952)] and [[2](https://arxiv.org/abs/2401.13875)], are out! 
- **[Jan 2024]** Two papers on Mixture of Experts, [[1](https://arxiv.org/abs/2305.07572)] and [2], are accepted to AISTATS 2024.
- **[Jan 2024]** Our paper "[Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://arxiv.org/abs/2309.13850)" is accepted to ICLR 2024.
- **[Dec 2023]** Our paper "[Fast Approximation of the Generalized Sliced-Wasserstein Distance](https://openreview.net/forum?id=u3JeFO8G8s)" is accepted to ICASSP 2024.
- **[Oct 2023]** I received the NeurIPS 2023 Scholar Award. See you in New Orleans this December!
- **[Oct 2023]** Our new paper "[A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts](https://arxiv.org/pdf/2310.14188.pdf)" is out.
- **[Sep 2023]** Our new paper "[Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://arxiv.org/pdf/2309.13850.pdf)" is out.
- **[Sep 2023]** We have two papers accepted to NeurIPS 2023, [[1](https://arxiv.org/pdf/2305.03288.pdf)] as <span style="color:red"> **spotlight** </span> and [[2](https://arxiv.org/pdf/2301.11808.pdf)] as poster.
- **[Jul 2023]** We will present the paper "[Fast Approximation of the Generalized Sliced-Wasserstein Distance](https://openreview.net/pdf?id=u3JeFO8G8s)" at the Frontier4LCD workshop, ICML 2023.
- **[May 2023]** Three new papers on the Mixture of Experts theory are out! See more at [[1]](https://arxiv.org/abs/2305.03288), [[2]](https://arxiv.org/abs/2305.07572) and [[3](https://huynm99.github.io/Deviated_MoE.pdf)].
- **[Feb 2023]** Our new paper on Mixture Models theory "[Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models](https://arxiv.org/abs/2301.11808)" is out.

## Professional Services
Conference Review: ICML (2022,2024), NeurIPS (2022-2023), AISTATS (2022-2024) and ICLR (2024).
Workshop Review: Frontier4LCD (ICML 2023).
